{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import DataGenerator as DG\n",
    "from DataGenerator import DataGenerator\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "#Progress bar fix: use callbacks=[Logger.JupyterProgbarLogger()] in model.fit\n",
    "#verbose=0 is also required\n",
    "import JupyterProgbarLogger as Logger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "\n",
    "###FIX NUMPY LOAD FOR DICTIONARIES\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "DATA_AMOUNT=650000\n",
    "VALIDATION_AMOUNT=40000\n",
    "#frames per sample / 3rd dimension for 3D CNN\n",
    "depth=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "                input_shape=(1,80, 80, 1),\n",
    "                stride_length=(1, 1, 1),\n",
    "                kernel=(3,3,3),\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                activation=layers.Activation('relu'),\n",
    "                dense_activation=layers.Activation('relu'),\n",
    "                output_activation=layers.Activation('softmax'),\n",
    "                batch_momentum=.999,\n",
    "                dropout_chance=0.2,\n",
    "                combine=True,\n",
    "                padding='same',\n",
    "                batch_norm=False,\n",
    "                dropout=False\n",
    "            ):\n",
    "    name = \"3D CNN\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    conv_parameters = {\n",
    "        'padding': padding,\n",
    "        'strides': stride_length,\n",
    "        'kernel_initializer': kernel_initializer,\n",
    "    }\n",
    "    # encode net\n",
    "    #if batch_norm:\n",
    "    #    x = layers.BatchNormalization(momentum=batch_momentum)(x)\n",
    "    #elif dropout:\n",
    "    #    x = layers.Dropout(dropout_chance)(x)\n",
    "    #x = activation(x)\n",
    "    x = layers.Conv3D(32, kernel, **conv_parameters)(x)\n",
    "    x = layers.MaxPooling3D((1, 2, 2), padding=padding)(x)\n",
    "    x = layers.Conv3D(64, kernel, **conv_parameters)(x)\n",
    "    x = layers.Conv3D(64, kernel, **conv_parameters)(x)\n",
    "    x = layers.MaxPooling3D((2, 2, 2), padding=padding)(x)\n",
    "    x = layers.Conv3D(128, kernel, **conv_parameters)(x)\n",
    "    x = layers.Conv3D(128, kernel, **conv_parameters)(x)\n",
    "    x = layers.MaxPooling3D((2, 2, 2), padding=padding)(x)\n",
    "    x = activation(x)\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    output = output_activation(x)\n",
    "    model = keras.models.Model(inputs, output)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(\n",
    "            learning_rate=1e-4,\n",
    "            momentum=0.9,\n",
    "            nesterov=True,\n",
    "            decay=1e-6\n",
    "        ),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 15:19:35.693381 140693000214336 deprecation.py:506] From /home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4, 80, 80, 1)]    0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 4, 80, 80, 32)     896       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 4, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 4, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 4, 40, 40, 64)     110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 2, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 2, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 2, 20, 20, 128)    442496    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling3d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 830,720\n",
      "Trainable params: 830,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if not depth == 1:\n",
    "    model,name = build_model(input_shape=(depth,80,80,1))\n",
    "else:\n",
    "    model,name = build_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0f95324f614c468779d8c238683f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: loss: 4.7253 - acc: 0.0614 - val_loss: 4.4878 - val_acc: 0.0680\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e267fa36f1374539b07be15daf1d33b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: loss: 3.8602 - acc: 0.0609 - val_loss: nan - val_acc: 0.0754\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8a7208807a4876a7b997cc83133c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: loss: nan - acc: 0.0192 - val_loss: 4.8520 - val_acc: 0.0143\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e8aa66044346a9bed219fd5645bf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: loss: nan - acc: 0.0178 - val_loss: nan - val_acc: 0.0130\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88dbff41a204fc1943cac40614c589e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: loss: 4.8520 - acc: 0.0132e-04\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-78:\n",
      "Process Keras_worker_ForkPoolWorker-84:\n",
      "Process Keras_worker_ForkPoolWorker-82:\n",
      "Process Keras_worker_ForkPoolWorker-77:\n",
      "Traceback (most recent call last):\n",
      "Process Keras_worker_ForkPoolWorker-72:\n",
      "Traceback (most recent call last):\n",
      "Process Keras_worker_ForkPoolWorker-73:\n",
      "Process Keras_worker_ForkPoolWorker-71:\n",
      "Process Keras_worker_ForkPoolWorker-75:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Process Keras_worker_ForkPoolWorker-74:\n",
      "Process Keras_worker_ForkPoolWorker-80:\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 571, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/ap487/2019-notebooks/Action Recognition/DataGenerator.py\", line 31, in __getitem__\n",
      "    return self.generate_chunk(index)\n",
      "  File \"/home/ap487/2019-notebooks/Action Recognition/DataGenerator.py\", line 60, in generate_chunk\n",
      "    chunk_data[i][cur_amount]=f[\"/frames/raw\"][index]\n",
      "KeyboardInterrupt\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/site-packages/h5py/_hl/dataset.py\", line 573, in __getitem__\n",
      "    self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl)\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: loss: 4.8520 - acc: 0.0132\r",
      "Metrics: loss: 4.8520 - acc: 0.0132\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ap487/miniconda3/envs/tf2_gpu/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8dc0765e0401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     callbacks=[Logger.JupyterProgbarLogger(),\n\u001b[0;32m---> 10\u001b[0;31m                               keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=10,verbose=1, mode='auto',restore_best_weights=True)]\n\u001b[0m\u001b[1;32m     11\u001b[0m                    )\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_gpu/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_gpu/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_gen = DataGenerator(\"images_raw_doric_round1.h5\",data_amount=DATA_AMOUNT,batch_size=BATCH_SIZE,frames_per_sample=depth)\n",
    "validation_gen = DataGenerator(\"images_raw_doric_round1.h5\",data_amount=VALIDATION_AMOUNT,batch_size=BATCH_SIZE,frames_per_sample=depth)#,offset=DATA_AMOUNT)\n",
    "history = model.fit_generator(generator=data_gen,\n",
    "                    validation_data=validation_gen,\n",
    "                    epochs=40,\n",
    "                    verbose=0,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=10,\n",
    "                    callbacks=[Logger.JupyterProgbarLogger(),\n",
    "                              keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=10,verbose=1, mode='auto',restore_best_weights=True)]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, answers):\n",
    "    accs = np.zeros(len(answers))\n",
    "    for i in range(0,len(answers)):\n",
    "        if(predictions[i]==answers[i]):\n",
    "            accs[i]=1\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(predicts,test_labels))\n",
    "print(metrics.f1_score(test_labels,predicts,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_benchmarks = np.load(\"benchmarks.npy\")\n",
    "model_benchmarks.item()[name]=metrics.f1_score(test_labels,predicts,average=None)\n",
    "np.save(\"benchmarks.npy\",model_benchmarks)\n",
    "print(model_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old Data Function -- using DataGenerator now.\n",
    "def sort_data(data,labels,train_len,val_len,test_len,blocksize=1):\n",
    "    curBlock = labels[0]\n",
    "    train_data = np.zeros((train_len,80,80,1))\n",
    "    val_data = np.zeros((val_len,80,80,1))\n",
    "    test_data = np.zeros((test_len,80,80,1))\n",
    "    train_labels = np.full(train_len,-1)\n",
    "    val_labels = np.full(val_len,-1)\n",
    "    test_labels = np.full(test_len,-1)\n",
    "    train_ind = 0\n",
    "    val_ind = 0\n",
    "    test_ind = 0\n",
    "    choices = np.arange(3)\n",
    "    i=0\n",
    "    cur_len = 0\n",
    "    while i < len(data):\n",
    "        start = i-cur_len\n",
    "        iter_amt = len(data) if blocksize == 1 else blocksize\n",
    "        while labels[i]==curBlock and cur_len < iter_amt:\n",
    "            i+=1\n",
    "            cur_len+=1\n",
    "            if i == len(data):\n",
    "                break\n",
    "        end = i\n",
    "        if not i == len(data):\n",
    "            if labels[i]==curBlock:\n",
    "                cur_len-=int(blocksize/2)\n",
    "            else:\n",
    "                cur_len=0\n",
    "            curBlock = labels[i]\n",
    "        if not blocksize == 1 and end-start < blocksize:\n",
    "            continue\n",
    "        cur_frames = data[start:end]\n",
    "        cur_labels = labels[start:end]\n",
    "        choice=-1\n",
    "        if choices.size>0:\n",
    "            choice = np.random.choice(choices)\n",
    "        else:\n",
    "            break\n",
    "        if choice == 0:\n",
    "            if train_ind + len(cur_frames) >= train_len:\n",
    "                cur_frames = cur_frames[:train_len-train_ind]\n",
    "                cur_labels = cur_labels[:train_len-train_ind]\n",
    "                choices=np.delete(choices,np.argwhere(choices==0))\n",
    "            train_data[train_ind:train_ind+len(cur_frames)]=cur_frames\n",
    "            train_labels[train_ind:train_ind+len(cur_frames)]=cur_labels\n",
    "            train_ind += len(cur_frames)\n",
    "        elif choice == 1:\n",
    "            if val_ind + len(cur_frames) >= val_len:\n",
    "                cur_frames = cur_frames[:val_len-val_ind]\n",
    "                cur_labels = cur_labels[:val_len-val_ind]\n",
    "                choices=np.delete(choices,np.argwhere(choices==1))\n",
    "            val_data[val_ind:val_ind+len(cur_frames)]=cur_frames\n",
    "            val_labels[val_ind:val_ind+len(cur_frames)]=cur_labels\n",
    "            val_ind += len(cur_frames)\n",
    "        elif choice == 2:\n",
    "            if test_ind + len(cur_frames) >= test_len:\n",
    "                cur_frames = cur_frames[:test_len-test_ind]\n",
    "                cur_labels = cur_labels[:test_len-test_ind]\n",
    "                choices=np.delete(choices,np.argwhere(choices==2))\n",
    "            test_data[test_ind:test_ind+len(cur_frames)]=cur_frames\n",
    "            test_labels[test_ind:test_ind+len(cur_frames)]=cur_labels\n",
    "            test_ind += len(cur_frames)\n",
    "    return train_data[:train_ind-(train_ind%blocksize)],train_labels[:train_ind-(train_ind%blocksize)],val_data[:val_ind-(val_ind%blocksize)],val_labels[:val_ind-(val_ind%blocksize)],test_data[:test_ind-(test_ind%blocksize)],test_labels[:test_ind-(test_ind%blocksize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing preprocessing to remove -5 labelled beforehand\n",
    "file_path = \"images_raw_doric_round1.h5\"\n",
    "with h5py.File(file_path,'r') as f:\n",
    "    with h5py.File(\"images_processed.h5\",'w') as w:\n",
    "        w.create_dataset(\"/frames/raw\",shape=f[\"/frames/raw\"].shape,dtype='uint8')\n",
    "        w.create_dataset(\"/labels\",data=f[\"/labels\"])\n",
    "        start = 0\n",
    "        index = 0\n",
    "        for i in range(0,len(f[\"/labels\"])):\n",
    "            if f[\"/labels\"][i]<0:\n",
    "                w[\"/frames/raw\"][index:index+i-start]=f[\"/frames/raw\"][start:i]\n",
    "                w[\"/labels\"][index:index+i-start]=f[\"/labels\"][start:i]\n",
    "                start=i\n",
    "                index+=i-start\n",
    "            if(i%1000==0):\n",
    "                print(\"Progress: \",i/(len(f[\"/frames/raw\"])),'            ',end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure data spread is normal, make sure to remove the [...,None] in DataGenerator.\n",
    "lbls = np.zeros(BATCH_SIZE*300)\n",
    "for i in range(0,300):\n",
    "    lbls[i*BATCH_SIZE:(i+1)*BATCH_SIZE]=data_gen.__getitem__(0)[1]\n",
    "plt.hist(lbls,46,density=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
