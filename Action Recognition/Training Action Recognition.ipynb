{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "#Progress bar fix: use callbacks=[TQDMNotebookCallback()] in model.fit\n",
    "#verbose=0 is also required\n",
    "import JupyterProgbarLogger as Logger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "DATA_AMOUNT=200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/amitp/Documents/Python/2019-notebooks/Action Recognition/images_raw_doric_round1.h5\"\n",
    "with h5py.File(file_path,'r') as f:\n",
    "    labels = f['/labels'][:DATA_AMOUNT]\n",
    "    data = f['/frames/raw'][:DATA_AMOUNT]\n",
    "data = data[(labels>=0)]\n",
    "labels = labels[(labels>=0)]\n",
    "if len(data.shape) < 4:\n",
    "    data=data[...,None]\n",
    "train_data=data[:int(DATA_AMOUNT*3/4)]\n",
    "train_labels=labels[:int(DATA_AMOUNT*3/4)]\n",
    "val_data=data[int(DATA_AMOUNT*3/4):int(DATA_AMOUNT*7/8)]\n",
    "val_labels=labels[int(DATA_AMOUNT*3/4):int(DATA_AMOUNT*7/8)]\n",
    "test_data=data[int(DATA_AMOUNT*7/8):DATA_AMOUNT]\n",
    "test_labels=labels[int(DATA_AMOUNT*7/8):DATA_AMOUNT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old-fashioned convnet for flip detection...\n",
    "def build_model(\n",
    "                input_shape=(80, 80, 1),\n",
    "                stride_length=(1, 1),\n",
    "                kernel=(4, 4),\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                activation=layers.Activation('relu'),\n",
    "                dense_activation=layers.Activation('relu'),\n",
    "                output_activation=layers.Activation('softmax'),\n",
    "                batch_momentum=.999,\n",
    "                combine=True,\n",
    "                padding='valid',\n",
    "                batch_norm=False\n",
    "            ):\n",
    "    nfilters=[4,8,16,32]\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    conv_parameters = {\n",
    "        'padding': padding,\n",
    "        'strides': stride_length,\n",
    "        'kernel_initializer': kernel_initializer\n",
    "    }\n",
    "\n",
    "    # encode net\n",
    "    for filters in nfilters:\n",
    "        x = layers.Conv2D(filters, kernel, **conv_parameters)(x)\n",
    "        x = layers.BatchNormalization(momentum=batch_momentum)(x)\n",
    "        x = activation(x)\n",
    "        x = layers.MaxPooling2D((2, 2), padding=padding)(x)\n",
    "    #x = layers.Conv2D(1, (1, 1))(x)\n",
    "    #x = activation(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(46)(x)\n",
    "    output = output_activation(x)\n",
    "\n",
    "    model = keras.models.Model(inputs, output)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(\n",
    "            learning_rate=1e-4,\n",
    "            momentum=0.9,\n",
    "            nesterov=True,\n",
    "            decay=1e-6\n",
    "        ),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 12:05:22.942905 140486872340288 deprecation.py:323] From /home/amitp/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287a12a7a09a4c69af4bd361c05cd6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=199994), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5344375db0db42f18c3703312613723f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=199994), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(data,\n",
    "                    labels,\n",
    "                    epochs=2,\n",
    "                    verbose=0,\n",
    "                    validation_data=(val_data,val_labels),\n",
    "                    callbacks=[Logger.JupyterProgbarLogger()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24994/24994 [==============================] - ETA: 12s - loss: 1.5624 - accuracy: 0.625 - ETA: 11s - loss: 1.9244 - accuracy: 0.475 - ETA: 11s - loss: 2.0868 - accuracy: 0.392 - ETA: 11s - loss: 1.6971 - accuracy: 0.519 - ETA: 11s - loss: 1.6993 - accuracy: 0.512 - ETA: 10s - loss: 1.7492 - accuracy: 0.531 - ETA: 10s - loss: 1.7946 - accuracy: 0.505 - ETA: 10s - loss: 1.8180 - accuracy: 0.495 - ETA: 10s - loss: 1.7859 - accuracy: 0.497 - ETA: 10s - loss: 1.8165 - accuracy: 0.468 - ETA: 9s - loss: 1.8295 - accuracy: 0.468 - ETA: 9s - loss: 1.8807 - accuracy: 0.45 - ETA: 9s - loss: 1.8974 - accuracy: 0.44 - ETA: 9s - loss: 1.9553 - accuracy: 0.41 - ETA: 9s - loss: 1.9909 - accuracy: 0.40 - ETA: 9s - loss: 1.9751 - accuracy: 0.41 - ETA: 9s - loss: 1.9893 - accuracy: 0.41 - ETA: 9s - loss: 1.9681 - accuracy: 0.42 - ETA: 9s - loss: 1.8806 - accuracy: 0.46 - ETA: 9s - loss: 1.8932 - accuracy: 0.45 - ETA: 9s - loss: 1.8637 - accuracy: 0.46 - ETA: 9s - loss: 1.8554 - accuracy: 0.46 - ETA: 9s - loss: 1.8681 - accuracy: 0.45 - ETA: 9s - loss: 1.8634 - accuracy: 0.45 - ETA: 9s - loss: 1.8811 - accuracy: 0.44 - ETA: 8s - loss: 1.8662 - accuracy: 0.45 - ETA: 8s - loss: 1.8950 - accuracy: 0.44 - ETA: 8s - loss: 1.9415 - accuracy: 0.43 - ETA: 8s - loss: 1.9190 - accuracy: 0.44 - ETA: 8s - loss: 1.9121 - accuracy: 0.44 - ETA: 8s - loss: 1.9255 - accuracy: 0.43 - ETA: 8s - loss: 1.9075 - accuracy: 0.44 - ETA: 8s - loss: 1.8980 - accuracy: 0.44 - ETA: 8s - loss: 1.9183 - accuracy: 0.43 - ETA: 8s - loss: 1.9107 - accuracy: 0.43 - ETA: 8s - loss: 1.9357 - accuracy: 0.43 - ETA: 8s - loss: 1.9440 - accuracy: 0.42 - ETA: 8s - loss: 1.9539 - accuracy: 0.42 - ETA: 8s - loss: 1.9564 - accuracy: 0.42 - ETA: 8s - loss: 1.9709 - accuracy: 0.41 - ETA: 8s - loss: 1.9693 - accuracy: 0.41 - ETA: 8s - loss: 1.9593 - accuracy: 0.41 - ETA: 7s - loss: 1.9816 - accuracy: 0.41 - ETA: 7s - loss: 1.9853 - accuracy: 0.41 - ETA: 7s - loss: 1.9922 - accuracy: 0.40 - ETA: 7s - loss: 1.9971 - accuracy: 0.40 - ETA: 7s - loss: 1.9929 - accuracy: 0.40 - ETA: 7s - loss: 1.9746 - accuracy: 0.41 - ETA: 7s - loss: 1.9716 - accuracy: 0.41 - ETA: 7s - loss: 1.9681 - accuracy: 0.41 - ETA: 7s - loss: 1.9476 - accuracy: 0.42 - ETA: 7s - loss: 1.9605 - accuracy: 0.42 - ETA: 7s - loss: 1.9729 - accuracy: 0.41 - ETA: 7s - loss: 1.9847 - accuracy: 0.41 - ETA: 7s - loss: 1.9898 - accuracy: 0.41 - ETA: 7s - loss: 1.9949 - accuracy: 0.40 - ETA: 7s - loss: 1.9928 - accuracy: 0.40 - ETA: 7s - loss: 1.9984 - accuracy: 0.40 - ETA: 7s - loss: 1.9942 - accuracy: 0.41 - ETA: 7s - loss: 2.0011 - accuracy: 0.40 - ETA: 7s - loss: 1.9838 - accuracy: 0.41 - ETA: 7s - loss: 1.9825 - accuracy: 0.41 - ETA: 6s - loss: 1.9840 - accuracy: 0.41 - ETA: 6s - loss: 1.9862 - accuracy: 0.41 - ETA: 6s - loss: 2.0014 - accuracy: 0.41 - ETA: 6s - loss: 1.9828 - accuracy: 0.42 - ETA: 6s - loss: 1.9969 - accuracy: 0.41 - ETA: 6s - loss: 2.0076 - accuracy: 0.41 - ETA: 6s - loss: 2.0202 - accuracy: 0.40 - ETA: 6s - loss: 2.0160 - accuracy: 0.40 - ETA: 6s - loss: 2.0092 - accuracy: 0.41 - ETA: 6s - loss: 2.0017 - accuracy: 0.41 - ETA: 6s - loss: 2.0035 - accuracy: 0.41 - ETA: 6s - loss: 2.0132 - accuracy: 0.41 - ETA: 6s - loss: 2.0177 - accuracy: 0.41 - ETA: 6s - loss: 2.0235 - accuracy: 0.40 - ETA: 6s - loss: 2.0248 - accuracy: 0.41 - ETA: 6s - loss: 2.0365 - accuracy: 0.40 - ETA: 6s - loss: 2.0396 - accuracy: 0.40 - ETA: 5s - loss: 2.0326 - accuracy: 0.40 - ETA: 5s - loss: 2.0385 - accuracy: 0.40 - ETA: 5s - loss: 2.0434 - accuracy: 0.40 - ETA: 5s - loss: 2.0377 - accuracy: 0.40 - ETA: 5s - loss: 2.0428 - accuracy: 0.40 - ETA: 5s - loss: 2.0386 - accuracy: 0.40 - ETA: 5s - loss: 2.0423 - accuracy: 0.40 - ETA: 5s - loss: 2.0460 - accuracy: 0.40 - ETA: 5s - loss: 2.0458 - accuracy: 0.40 - ETA: 5s - loss: 2.0465 - accuracy: 0.40 - ETA: 5s - loss: 2.0514 - accuracy: 0.40 - ETA: 5s - loss: 2.0524 - accuracy: 0.40 - ETA: 5s - loss: 2.0554 - accuracy: 0.40 - ETA: 5s - loss: 2.0612 - accuracy: 0.40 - ETA: 5s - loss: 2.0690 - accuracy: 0.39 - ETA: 5s - loss: 2.0637 - accuracy: 0.40 - ETA: 5s - loss: 2.0723 - accuracy: 0.39 - ETA: 5s - loss: 2.0799 - accuracy: 0.39 - ETA: 5s - loss: 2.0949 - accuracy: 0.39 - ETA: 4s - loss: 2.0994 - accuracy: 0.39 - ETA: 4s - loss: 2.0932 - accuracy: 0.39 - ETA: 4s - loss: 2.0967 - accuracy: 0.39 - ETA: 4s - loss: 2.0934 - accuracy: 0.39 - ETA: 4s - loss: 2.0791 - accuracy: 0.40 - ETA: 4s - loss: 2.0858 - accuracy: 0.40 - ETA: 4s - loss: 2.0969 - accuracy: 0.39 - ETA: 4s - loss: 2.0974 - accuracy: 0.40 - ETA: 4s - loss: 2.0995 - accuracy: 0.40 - ETA: 4s - loss: 2.1091 - accuracy: 0.39 - ETA: 4s - loss: 2.1109 - accuracy: 0.39 - ETA: 4s - loss: 2.1207 - accuracy: 0.39 - ETA: 4s - loss: 2.1249 - accuracy: 0.39 - ETA: 4s - loss: 2.1287 - accuracy: 0.39 - ETA: 4s - loss: 2.1245 - accuracy: 0.39 - ETA: 4s - loss: 2.1252 - accuracy: 0.38 - ETA: 4s - loss: 2.1217 - accuracy: 0.39 - ETA: 4s - loss: 2.1197 - accuracy: 0.39 - ETA: 3s - loss: 2.1156 - accuracy: 0.39 - ETA: 3s - loss: 2.1157 - accuracy: 0.39 - ETA: 3s - loss: 2.1130 - accuracy: 0.39 - ETA: 3s - loss: 2.1107 - accuracy: 0.39 - ETA: 3s - loss: 2.1108 - accuracy: 0.39 - ETA: 3s - loss: 2.1170 - accuracy: 0.39 - ETA: 3s - loss: 2.1176 - accuracy: 0.39 - ETA: 3s - loss: 2.1197 - accuracy: 0.39 - ETA: 3s - loss: 2.1268 - accuracy: 0.38 - ETA: 3s - loss: 2.1241 - accuracy: 0.39 - ETA: 3s - loss: 2.1149 - accuracy: 0.39 - ETA: 3s - loss: 2.1177 - accuracy: 0.39 - ETA: 3s - loss: 2.1166 - accuracy: 0.39 - ETA: 3s - loss: 2.1204 - accuracy: 0.39 - ETA: 3s - loss: 2.1174 - accuracy: 0.39 - ETA: 3s - loss: 2.1195 - accuracy: 0.39 - ETA: 3s - loss: 2.1225 - accuracy: 0.39 - ETA: 3s - loss: 2.1303 - accuracy: 0.38 - ETA: 2s - loss: 2.1327 - accuracy: 0.38 - ETA: 2s - loss: 2.1369 - accuracy: 0.38 - ETA: 2s - loss: 2.1327 - accuracy: 0.38 - ETA: 2s - loss: 2.1372 - accuracy: 0.38 - ETA: 2s - loss: 2.1366 - accuracy: 0.38 - ETA: 2s - loss: 2.1347 - accuracy: 0.38 - ETA: 2s - loss: 2.1351 - accuracy: 0.38 - ETA: 2s - loss: 2.1318 - accuracy: 0.38 - ETA: 2s - loss: 2.1372 - accuracy: 0.38 - ETA: 2s - loss: 2.1358 - accuracy: 0.38 - ETA: 2s - loss: 2.1338 - accuracy: 0.38 - ETA: 2s - loss: 2.1387 - accuracy: 0.38 - ETA: 2s - loss: 2.1406 - accuracy: 0.38 - ETA: 2s - loss: 2.1444 - accuracy: 0.38 - ETA: 2s - loss: 2.1493 - accuracy: 0.38 - ETA: 2s - loss: 2.1484 - accuracy: 0.37 - ETA: 2s - loss: 2.1522 - accuracy: 0.37 - ETA: 2s - loss: 2.1477 - accuracy: 0.38 - ETA: 2s - loss: 2.1501 - accuracy: 0.37 - ETA: 1s - loss: 2.1545 - accuracy: 0.37 - ETA: 1s - loss: 2.1519 - accuracy: 0.37 - ETA: 1s - loss: 2.1566 - accuracy: 0.37 - ETA: 1s - loss: 2.1540 - accuracy: 0.37 - ETA: 1s - loss: 2.1553 - accuracy: 0.37 - ETA: 1s - loss: 2.1543 - accuracy: 0.37 - ETA: 1s - loss: 2.1542 - accuracy: 0.37 - ETA: 1s - loss: 2.1508 - accuracy: 0.37 - ETA: 1s - loss: 2.1450 - accuracy: 0.37 - ETA: 1s - loss: 2.1472 - accuracy: 0.37 - ETA: 1s - loss: 2.1479 - accuracy: 0.37 - ETA: 1s - loss: 2.1445 - accuracy: 0.37 - ETA: 1s - loss: 2.1459 - accuracy: 0.37 - ETA: 1s - loss: 2.1415 - accuracy: 0.38 - ETA: 1s - loss: 2.1391 - accuracy: 0.38 - ETA: 1s - loss: 2.1433 - accuracy: 0.38 - ETA: 1s - loss: 2.1396 - accuracy: 0.38 - ETA: 1s - loss: 2.1294 - accuracy: 0.38 - ETA: 1s - loss: 2.1274 - accuracy: 0.38 - ETA: 0s - loss: 2.1257 - accuracy: 0.38 - ETA: 0s - loss: 2.1269 - accuracy: 0.38 - ETA: 0s - loss: 2.1278 - accuracy: 0.38 - ETA: 0s - loss: 2.1248 - accuracy: 0.38 - ETA: 0s - loss: 2.1275 - accuracy: 0.38 - ETA: 0s - loss: 2.1315 - accuracy: 0.38 - ETA: 0s - loss: 2.1320 - accuracy: 0.38 - ETA: 0s - loss: 2.1319 - accuracy: 0.38 - ETA: 0s - loss: 2.1351 - accuracy: 0.38 - ETA: 0s - loss: 2.1335 - accuracy: 0.38 - ETA: 0s - loss: 2.1351 - accuracy: 0.38 - ETA: 0s - loss: 2.1391 - accuracy: 0.38 - ETA: 0s - loss: 2.1387 - accuracy: 0.38 - ETA: 0s - loss: 2.1358 - accuracy: 0.38 - ETA: 0s - loss: 2.1422 - accuracy: 0.37 - ETA: 0s - loss: 2.1419 - accuracy: 0.38 - ETA: 0s - loss: 2.1398 - accuracy: 0.38 - ETA: 0s - loss: 2.1451 - accuracy: 0.37 - ETA: 0s - loss: 2.1440 - accuracy: 0.37 - 10s 413us/sample - loss: 2.1438 - accuracy: 0.3793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1438371247078423, 0.37933105]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
