{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 17 15:31:26 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce MX150       Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   61C    P0    N/A /  N/A |    381MiB /  2002MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1209      G   /usr/lib/xorg/Xorg                           229MiB |\r\n",
      "|    0      2056      G   /usr/bin/gnome-shell                         151MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "#Progress bar fix: use callbacks=[Logger.JupyterProgbarLogger()] in model.fit\n",
    "#verbose=0 is also required\n",
    "import JupyterProgbarLogger as Logger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "\n",
    "###FIX NUMPY LOAD FOR DICTIONARIES\\\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "DATA_AMOUNT=300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(data,labels,train_len,val_len,test_len,blocksize=1):\n",
    "    curBlock = labels[0]\n",
    "    train_data = np.zeros((train_len,80,80,1))\n",
    "    val_data = np.zeros((val_len,80,80,1))\n",
    "    test_data = np.zeros((test_len,80,80,1))\n",
    "    train_labels = np.full(train_len,-1)\n",
    "    val_labels = np.full(val_len,-1)\n",
    "    test_labels = np.full(test_len,-1)\n",
    "    train_ind = 0\n",
    "    val_ind = 0\n",
    "    test_ind = 0\n",
    "    choices = np.arange(3)\n",
    "    i=0\n",
    "    cur_len = 0\n",
    "    while i < len(data):\n",
    "        start = i-cur_len\n",
    "        iter_amt = len(data) if blocksize == 1 else blocksize\n",
    "        while labels[i]==curBlock and cur_len < iter_amt:\n",
    "            i+=1\n",
    "            cur_len+=1\n",
    "            if i == len(data):\n",
    "                break\n",
    "        end = i\n",
    "        if not i == len(data):\n",
    "            if labels[i]==curBlock:\n",
    "                cur_len-=int(blocksize/2)\n",
    "            else:\n",
    "                cur_len=0\n",
    "            curBlock = labels[i]\n",
    "        if not blocksize == 1 and end-start < blocksize:\n",
    "            continue\n",
    "        cur_frames = data[start:end]\n",
    "        cur_labels = labels[start:end]\n",
    "        choice=-1\n",
    "        if choices.size>0:\n",
    "            choice = np.random.choice(choices)\n",
    "        else:\n",
    "            break\n",
    "        if choice == 0:\n",
    "            if train_ind + len(cur_frames) >= train_len:\n",
    "                cur_frames = cur_frames[:train_len-train_ind]\n",
    "                cur_labels = cur_labels[:train_len-train_ind]\n",
    "                choices=np.delete(choices,np.argwhere(choices==0))\n",
    "            train_data[train_ind:train_ind+len(cur_frames)]=cur_frames\n",
    "            train_labels[train_ind:train_ind+len(cur_frames)]=cur_labels\n",
    "            train_ind += len(cur_frames)\n",
    "        elif choice == 1:\n",
    "            if val_ind + len(cur_frames) >= val_len:\n",
    "                cur_frames = cur_frames[:val_len-val_ind]\n",
    "                cur_labels = cur_labels[:val_len-val_ind]\n",
    "                choices=np.delete(choices,np.argwhere(choices==1))\n",
    "            val_data[val_ind:val_ind+len(cur_frames)]=cur_frames\n",
    "            val_labels[val_ind:val_ind+len(cur_frames)]=cur_labels\n",
    "            val_ind += len(cur_frames)\n",
    "        elif choice == 2:\n",
    "            if test_ind + len(cur_frames) >= test_len:\n",
    "                cur_frames = cur_frames[:test_len-test_ind]\n",
    "                cur_labels = cur_labels[:test_len-test_ind]\n",
    "                choices=np.delete(choices,np.argwhere(choices==2))\n",
    "            test_data[test_ind:test_ind+len(cur_frames)]=cur_frames\n",
    "            test_labels[test_ind:test_ind+len(cur_frames)]=cur_labels\n",
    "            test_ind += len(cur_frames)\n",
    "    return train_data[:train_ind-(train_ind%blocksize)],train_labels[:train_ind-(train_ind%blocksize)],val_data[:val_ind-(val_ind%blocksize)],val_labels[:val_ind-(val_ind%blocksize)],test_data[:test_ind-(test_ind%blocksize)],test_labels[:test_ind-(test_ind%blocksize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"images_raw_doric_round1.h5\"\n",
    "with h5py.File(file_path,'r') as f:\n",
    "    labels = f['/labels'][:DATA_AMOUNT]\n",
    "    data = f['/frames/raw'][:DATA_AMOUNT]\n",
    "data = data[(labels>=0)]\n",
    "labels = labels[(labels>=0)]\n",
    "DATA_AMOUNT=len(data)\n",
    "if len(data.shape) < 4:\n",
    "    data=data[...,None]\n",
    "tl = math.floor(DATA_AMOUNT/2)\n",
    "vl=tsl=math.floor(DATA_AMOUNT/4)\n",
    "depth=32\n",
    "train_data,train_labels,val_data,val_labels,test_data,test_labels = sort_data(data,labels,tl,vl,tsl,blocksize=depth)\n",
    "del data\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "                input_shape=(1,80, 80, 1),\n",
    "                stride_length=(1, 1, 1),\n",
    "                kernel=(3,3,3),\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                activation=layers.Activation('relu'),\n",
    "                dense_activation=layers.Activation('relu'),\n",
    "                output_activation=layers.Activation('softmax'),\n",
    "                batch_momentum=.999,\n",
    "                dropout_chance=0.2,\n",
    "                combine=True,\n",
    "                padding='valid',\n",
    "                batch_norm=False,\n",
    "                dropout=False\n",
    "            ):\n",
    "    name = \"3D CNN\"\n",
    "    nfilters=[32,64,128]\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "\n",
    "    conv_parameters = {\n",
    "        'padding': padding,\n",
    "        'strides': stride_length,\n",
    "        'kernel_initializer': kernel_initializer\n",
    "    }\n",
    "\n",
    "\n",
    "    # encode net\n",
    "    for filters in nfilters:\n",
    "        x = layers.Conv3D(filters, kernel, **conv_parameters)(x)\n",
    "        if batch_norm:\n",
    "            x = layers.BatchNormalization(momentum=batch_momentum)(x)\n",
    "        elif dropout:\n",
    "            x = layers.Dropout(dropout_chance)(x)\n",
    "        #x = activation(x)\n",
    "        x = layers.MaxPooling3D((2, 2, 2), padding=padding)(x)\n",
    "    #x = layers.Conv2D(1, (1, 1))(x)\n",
    "    x = activation(x)\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    output = output_activation(x)\n",
    "\n",
    "\n",
    "    model = keras.models.Model(inputs, output)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(\n",
    "            learning_rate=1e-3,\n",
    "            momentum=0.9,\n",
    "            nesterov=True,\n",
    "            decay=1e-6\n",
    "        ),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not depth == 1:\n",
    "    model,name = build_model(input_shape=(depth,80,80,1))\n",
    "    if(len(train_data.shape)<5):\n",
    "        train_data=np.reshape(train_data,(int(len(train_data)/depth),depth,80,80,1))\n",
    "        val_data=np.reshape(val_data,(int(len(val_data)/depth),depth,80,80,1))\n",
    "        test_data=np.reshape(test_data,(int(len(test_data)/depth),depth,80,80,1))\n",
    "        train_labels=train_labels[::depth]\n",
    "        val_labels=val_labels[::depth]\n",
    "        test_labels=test_labels[::depth]\n",
    "else:\n",
    "    model,name = build_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    verbose=0,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(val_data,val_labels),\n",
    "                    callbacks=[Logger.JupyterProgbarLogger()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, answers):\n",
    "    accs = np.zeros(len(answers))\n",
    "    for i in range(0,len(answers)):\n",
    "        if(predictions[i]==answers[i]):\n",
    "            accs[i]=1\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = np.argmax(model.predict(test_data),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist((test_labels,predicts), density=True, histtype='bar', stacked=True, label = (\"Data\",\"Predictions\"))\n",
    "#plt.legend()\n",
    "plt.plot(test_labels)\n",
    "plt.plot(predicts)\n",
    "plt.xlim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(predicts,test_labels))\n",
    "print(metrics.f1_score(test_labels,predicts,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_benchmarks = np.load(\"benchmarks.npy\")\n",
    "model_benchmarks.item()[name]=metrics.f1_score(test_labels,predicts,average=None)\n",
    "np.save(\"benchmarks.npy\",model_benchmarks)\n",
    "print(model_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(labels)\n",
    "ind = 500\n",
    "plt.xlim(ind,ind+10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((train_labels,val_labels,test_labels),density=True)\n",
    "#plt.hist(train_labels,46,density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.client.device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3d00d838479b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_available_gpus\u001b[0;34m()\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_LOCAL_DEVICES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GPU'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mdefault_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdefault_session\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_session'"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
